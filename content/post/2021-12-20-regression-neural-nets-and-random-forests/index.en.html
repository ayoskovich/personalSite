---
title: Regression, Neural Nets, and Random Forests
author: R package build
date: '2021-12-20'
slug: []
categories: []
tags: []
subtitle: ''
summary: 'Implementing them in R.'
authors: []
lastmod: '2021-12-20T17:02:48-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  chunk_output_type: console
draft: true
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<pre class="r"><code>library(tidymodels)</code></pre>
<pre><code>## Warning: package &#39;tidymodels&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;tune&#39;:
##   method                   from   
##   required_pkgs.model_spec parsnip</code></pre>
<pre><code>## -- Attaching packages -------------------------------------- tidymodels 0.1.4 --</code></pre>
<pre><code>## v broom        0.7.10     v recipes      0.1.17
## v dials        0.0.10     v rsample      0.1.1 
## v dplyr        1.0.7      v tibble       3.1.6 
## v ggplot2      3.3.5      v tidyr        1.1.4 
## v infer        1.0.0      v tune         0.1.6 
## v modeldata    0.1.1      v workflows    0.2.4 
## v parsnip      0.1.7      v workflowsets 0.1.0 
## v purrr        0.3.4      v yardstick    0.0.9</code></pre>
<pre><code>## Warning: package &#39;broom&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;dials&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;scales&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;infer&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;modeldata&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;parsnip&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;recipes&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;rsample&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;tidyr&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;tune&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;workflows&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;workflowsets&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Warning: package &#39;yardstick&#39; was built under R version 4.0.5</code></pre>
<pre><code>## -- Conflicts ----------------------------------------- tidymodels_conflicts() --
## x purrr::discard() masks scales::discard()
## x dplyr::filter()  masks stats::filter()
## x dplyr::lag()     masks stats::lag()
## x recipes::step()  masks stats::step()
## * Learn how to get started at https://www.tidymodels.org/start/</code></pre>
<pre class="r"><code># Class (PS: Poorly segmented, WS: Well segmented)
data(cells)</code></pre>
<div id="baseline-64" class="section level1">
<h1>Baseline (64%)</h1>
<pre class="r"><code>library(janitor)</code></pre>
<pre><code>## Warning: package &#39;janitor&#39; was built under R version 4.0.5</code></pre>
<pre><code>## 
## Attaching package: &#39;janitor&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     chisq.test, fisher.test</code></pre>
<pre class="r"><code>cells %&gt;%
  tabyl(class)</code></pre>
<pre><code>##  class    n   percent
##     PS 1300 0.6438831
##     WS  719 0.3561169</code></pre>
</div>
<div id="logistic-regression-the-old-school-way-76.5" class="section level1">
<h1>Logistic Regression the old school way (76.5%)</h1>
<ul>
<li>Sensitivity: true positive rate</li>
<li>Specificity: true negative rate</li>
</ul>
<pre class="r"><code>library(pROC)</code></pre>
<pre><code>## Warning: package &#39;pROC&#39; was built under R version 4.0.5</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre class="r"><code>library(glue)</code></pre>
<pre><code>## Warning: package &#39;glue&#39; was built under R version 4.0.5</code></pre>
<pre class="r"><code>cells &lt;- cells %&gt;%
  mutate_at(c(&#39;class&#39;), factor)

mymod &lt;- glm(
  class ~ area_ch_1+area_ch_1+skew_inten_ch_3+skew_inten_ch_1+fiber_length_ch_1, 
  family=&#39;binomial&#39;, 
  data=cells
)

mymod %&gt;% summary()</code></pre>
<pre><code>## 
## Call:
## glm(formula = class ~ area_ch_1 + area_ch_1 + skew_inten_ch_3 + 
##     skew_inten_ch_1 + fiber_length_ch_1, family = &quot;binomial&quot;, 
##     data = cells)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.2891  -0.7903  -0.4584   0.8942   3.4500  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        0.0046045  0.1356404   0.034    0.973    
## area_ch_1          0.0069622  0.0005357  12.996  &lt; 2e-16 ***
## skew_inten_ch_3    0.2975305  0.0555260   5.358  8.4e-08 ***
## skew_inten_ch_1   -0.8249383  0.0982923  -8.393  &lt; 2e-16 ***
## fiber_length_ch_1 -0.0848630  0.0062673 -13.541  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2629.3  on 2018  degrees of freedom
## Residual deviance: 2068.3  on 2014  degrees of freedom
## AIC: 2078.3
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>preds &lt;- predict(mymod, newdata = cells, type=&#39;response&#39;)

aug_cells &lt;- cells %&gt;%
  bind_cols(list(the_preds = preds))
  
analysis &lt;- roc(aug_cells$class, aug_cells$the_preds)</code></pre>
<pre><code>## Setting levels: control = PS, case = WS</code></pre>
<pre><code>## Setting direction: controls &lt; cases</code></pre>
<pre class="r"><code># Maximize the sum of sensitivity and specificity
e &lt;- cbind(
  analysis$thresholds,
  analysis$sensitivities+analysis$specificities
)
opt_t &lt;- subset(
  e, 
  e[,2]==max(e[,2])
)[,1]

plot(
  1-analysis$specificities,analysis$sensitivities,type=&quot;l&quot;,
  ylab=&quot;Sensitiviy&quot;,
  xlab=&quot;1-Specificity&quot;,
  col=&quot;black&quot;,
  lwd=2,
  main = glue(&quot;ROC Curve (Optimal Thresh: {round(opt_t, 4)})&quot;)
)

abline(a=0,b=1)
abline(v = opt_t) #add optimal t to ROC curve</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>aug_cells %&gt;%
  mutate(final = if_else(the_preds &gt;= opt_t, &#39;WS&#39;, &#39;PS&#39;)) %&gt;%
  summarise(mean(final == class))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `mean(final == class)`
##                    &lt;dbl&gt;
## 1                  0.765</code></pre>
<pre class="r"><code>aug_cells %&gt;%
  mutate(final = if_else(the_preds &gt;= opt_t, &#39;WS&#39;, &#39;PS&#39;)) %&gt;%
  tabyl(class, final) %&gt;%
  adorn_percentages(denominator = &#39;all&#39;)</code></pre>
<pre><code>##  class         PS        WS
##     PS 0.48687469 0.1570084
##     WS 0.07825656 0.2778603</code></pre>
</div>
<div id="again-with-tidymodels" class="section level1">
<h1>Again with tidymodels</h1>
<pre class="r"><code># Recipe
myrec &lt;- cells %&gt;% 
  select(
    -case
  ) %&gt;% 
  recipe(class ~ .)

# Model (THIS IS WHERE PARSNIP IS REALLY GREAT)
# mymod &lt;- parsnip::logistic_reg() %&gt;%
#    set_engine(&#39;glm&#39;)
mymod &lt;- parsnip::rand_forest() %&gt;%
  set_mode(&#39;classification&#39;) %&gt;%
  set_engine(&#39;ranger&#39;)
#mymod &lt;- parsnip::mlp() %&gt;%
#  set_mode(&#39;classification&#39;) %&gt;%
#  set_engine(&#39;nnet&#39;)

# Fit the model using the recipe
myfit &lt;- workflow() %&gt;%
  add_recipe(myrec) %&gt;%
  add_model(mymod) %&gt;%
  fit(cells)

# Predict and append the predictions using the fitted model
new_preds &lt;- myfit %&gt;%
  predict(
    new_data = cells
  ) %&gt;%
  bind_cols(cells)</code></pre>
<div id="assess-model-performance-82" class="section level2">
<h2>Assess model performance (82%)</h2>
<pre class="r"><code># Let parsnip decide
new_preds %&gt;%
  metrics(truth = class, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.992
## 2 kap      binary         0.982</code></pre>
<pre class="r"><code># Confusion matrix
new_preds %&gt;%
  tabyl(class, .pred_class) %&gt;%
  adorn_percentages(denominator = &#39;all&#39;)</code></pre>
<pre><code>##  class          PS          WS
##     PS 0.641901932 0.001981179
##     WS 0.006438831 0.349678058</code></pre>
<pre class="r"><code># Accuracy
new_preds %&gt;%
  summarise(accur = mean(.pred_class == class))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   accur
##   &lt;dbl&gt;
## 1 0.992</code></pre>
</div>
</div>
<div id="simplifying-via-parsnip" class="section level1">
<h1>Simplifying via parsnip</h1>
<pre class="r"><code># Model (THIS IS WHERE PARSNIP IS REALLY GREAT)
a_log &lt;- parsnip::logistic_reg() %&gt;%
   set_engine(&#39;glm&#39;)
a_for &lt;- parsnip::rand_forest() %&gt;%
  set_mode(&#39;classification&#39;) %&gt;%
  set_engine(&#39;ranger&#39;)
a_nn &lt;- parsnip::mlp() %&gt;%
  set_mode(&#39;classification&#39;) %&gt;%
  set_engine(&#39;nnet&#39;)

generate &lt;- function(mylab, recy, mody, train_dat, test_dat){
  workflow() %&gt;%
    add_recipe(recy) %&gt;%
    add_model(mody) %&gt;%
    fit(train_dat) %&gt;%
    predict(
      new_data = test_dat
    ) %&gt;%
    bind_cols(test_dat) %&gt;%
    metrics(
      truth=&#39;class&#39;, 
      estimate=.pred_class
    ) %&gt;%
    mutate(meta_lab = mylab)
}

cell_split &lt;- initial_split(cells, prop=.75)
cell_train &lt;- training(cell_split)
cell_test &lt;- testing(cell_split)
  
final_results &lt;- bind_rows(
  generate(&#39;simple logistic&#39;, myrec, a_log, cell_train, cell_test),
  generate(&#39;random forest&#39;, myrec, a_for, cell_train, cell_test),
  generate(&#39;neural net&#39;, myrec, a_nn, cell_train, cell_test)
)</code></pre>
<pre><code>## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :
## prediction from a rank-deficient fit may be misleading</code></pre>
</div>
<div id="visualize-final-results" class="section level1">
<h1>Visualize final results</h1>
<pre class="r"><code>final_results %&gt;%
  ggplot(aes(meta_lab, .estimate, fill=.metric))+
  geom_col(position = &#39;dodge&#39;) +
  geom_text(
    aes(
      y=.estimate+.02,
      label=percent(.estimate, accuracy=.1)
    ), 
    position=position_dodge(width=1)
  )+
  geom_hline(aes(yintercept=.64), size=1.5)+
  scale_y_continuous(
    limits=c(0, 1.05), breaks=seq(0, 1, .5),
    labels=scales::percent_format(accuracy=2),
    expand = expansion(add=c(0, .1))
  )+
  scale_fill_brewer(palette=&#39;Set2&#39;)+
  ggthemes::theme_clean() +
  labs(
    x=&#39;Model&#39;, y=&#39;Accuracy&#39;,
    title=&#39;Model Accuracies&#39;
  )+
  theme(
    axis.ticks.x = element_blank()
  )</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>

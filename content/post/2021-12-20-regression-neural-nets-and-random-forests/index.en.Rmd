---
title: Regression, Neural Nets, and Random Forests
author: R package build
date: '2021-12-20'
slug: []
categories: []
tags: []
subtitle: ''
summary: 'Implementing them in R.'
authors: []
lastmod: '2021-12-20T17:02:48-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  chunk_output_type: console
draft: true
---

```{r}
library(tidymodels)

# Class (PS: Poorly segmented, WS: Well segmented)
data(cells)
```


# Baseline (64%)
```{r}
library(janitor)
cells %>%
  tabyl(class)
```

# Logistic Regression the old school way (76.5%)
- Sensitivity: true positive rate
- Specificity: true negative rate
```{r}
library(pROC)
library(glue)

cells <- cells %>%
  mutate_at(c('class'), factor)

mymod <- glm(
  class ~ area_ch_1+area_ch_1+skew_inten_ch_3+skew_inten_ch_1+fiber_length_ch_1, 
  family='binomial', 
  data=cells
)

mymod %>% summary()

preds <- predict(mymod, newdata = cells, type='response')

aug_cells <- cells %>%
  bind_cols(list(the_preds = preds))
  
analysis <- roc(aug_cells$class, aug_cells$the_preds)

# Maximize the sum of sensitivity and specificity
e <- cbind(
  analysis$thresholds,
  analysis$sensitivities+analysis$specificities
)
opt_t <- subset(
  e, 
  e[,2]==max(e[,2])
)[,1]

plot(
  1-analysis$specificities,analysis$sensitivities,type="l",
  ylab="Sensitiviy",
  xlab="1-Specificity",
  col="black",
  lwd=2,
  main = glue("ROC Curve (Optimal Thresh: {round(opt_t, 4)})")
)

abline(a=0,b=1)
abline(v = opt_t) #add optimal t to ROC curve
```

```{r}
aug_cells %>%
  mutate(final = if_else(the_preds >= opt_t, 'WS', 'PS')) %>%
  summarise(mean(final == class))

aug_cells %>%
  mutate(final = if_else(the_preds >= opt_t, 'WS', 'PS')) %>%
  tabyl(class, final) %>%
  adorn_percentages(denominator = 'all')
```

# Again with tidymodels
```{r}
# Recipe
myrec <- cells %>% 
  select(
    -case
  ) %>% 
  recipe(class ~ .)

# Model (THIS IS WHERE PARSNIP IS REALLY GREAT)
# mymod <- parsnip::logistic_reg() %>%
#    set_engine('glm')
mymod <- parsnip::rand_forest() %>%
  set_mode('classification') %>%
  set_engine('ranger')
#mymod <- parsnip::mlp() %>%
#  set_mode('classification') %>%
#  set_engine('nnet')

# Fit the model using the recipe
myfit <- workflow() %>%
  add_recipe(myrec) %>%
  add_model(mymod) %>%
  fit(cells)

# Predict and append the predictions using the fitted model
new_preds <- myfit %>%
  predict(
    new_data = cells
  ) %>%
  bind_cols(cells)
```

## Assess model performance (82%)
```{r}
# Let parsnip decide
new_preds %>%
  metrics(truth = class, estimate = .pred_class)

# Confusion matrix
new_preds %>%
  tabyl(class, .pred_class) %>%
  adorn_percentages(denominator = 'all')

# Accuracy
new_preds %>%
  summarise(accur = mean(.pred_class == class))
```

# Simplifying via parsnip
```{r}
# Model (THIS IS WHERE PARSNIP IS REALLY GREAT)
a_log <- parsnip::logistic_reg() %>%
   set_engine('glm')
a_for <- parsnip::rand_forest() %>%
  set_mode('classification') %>%
  set_engine('ranger')
a_nn <- parsnip::mlp() %>%
  set_mode('classification') %>%
  set_engine('nnet')

generate <- function(mylab, recy, mody, train_dat, test_dat){
  workflow() %>%
    add_recipe(recy) %>%
    add_model(mody) %>%
    fit(train_dat) %>%
    predict(
      new_data = test_dat
    ) %>%
    bind_cols(test_dat) %>%
    metrics(
      truth='class', 
      estimate=.pred_class
    ) %>%
    mutate(meta_lab = mylab)
}

cell_split <- initial_split(cells, prop=.75)
cell_train <- training(cell_split)
cell_test <- testing(cell_split)
  
final_results <- bind_rows(
  generate('simple logistic', myrec, a_log, cell_train, cell_test),
  generate('random forest', myrec, a_for, cell_train, cell_test),
  generate('neural net', myrec, a_nn, cell_train, cell_test)
)
```

# Visualize final results
```{r}
final_results %>%
  ggplot(aes(meta_lab, .estimate, fill=.metric))+
  geom_col(position = 'dodge') +
  geom_text(
    aes(
      y=.estimate+.02,
      label=percent(.estimate, accuracy=.1)
    ), 
    position=position_dodge(width=1)
  )+
  geom_hline(aes(yintercept=.64), size=1.5)+
  scale_y_continuous(
    limits=c(0, 1.05), breaks=seq(0, 1, .5),
    labels=scales::percent_format(accuracy=2),
    expand = expansion(add=c(0, .1))
  )+
  scale_fill_brewer(palette='Set2')+
  ggthemes::theme_clean() +
  labs(
    x='Model', y='Accuracy',
    title='Model Accuracies'
  )+
  theme(
    axis.ticks.x = element_blank()
  )
```
